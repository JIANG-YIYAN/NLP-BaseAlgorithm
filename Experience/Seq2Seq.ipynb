{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc12d3b5-9600-4d42-9dcb-950a38a5b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.728 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import data\n",
    "import jieba\n",
    "\n",
    "def tokenizer(text):\n",
    "    token = [tok for tok in jieba.cut(text)]\n",
    "    return token\n",
    "\n",
    "# Field对象指定你想要怎么处理某个数据\n",
    "TEXT = data.Field(tokenize=tokenizer,\n",
    "                  init_token='<sos>',\n",
    "                  eos_token='<eos>',\n",
    "                  lower=True,\n",
    "                  batch_first=True)\n",
    "\n",
    "# train,train.shape len(train) = 49000 <torchtext.legacy.data.dataset.TabularDataset at 0x1d8cac0ea00>\n",
    "# val,val.shape len(val) = 1000 <torchtext.legacy.data.dataset.TabularDataset at 0x1d8cac0e9d0>\n",
    "# 告诉Fields去处理哪些数据\n",
    "train, val = data.TabularDataset.splits(\n",
    "    path='./data/',\n",
    "    train='train.tsv',\n",
    "    validation='dev.tsv',\n",
    "    format='tsv',\n",
    "    skip_header=True,\n",
    "    fields=[('trg', TEXT), ('src', TEXT)])\n",
    "\n",
    "# train_iter 自动shuffle, val_iter 按照sort_key排序\n",
    "# train_iter = 192, val_iter = 4\n",
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "    (train, val),\n",
    "    batch_sizes=(64, 64),\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21315f7d-4968-4021-92d2-947b68c88b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(train, min_freq=2)\n",
    "# vocab 249536\n",
    "vocab = TEXT.vocab\n",
    "# dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
    "vocab.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35090d1e-5b9f-406f-99c5-3d15aac9a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2vocab: 249536 里面只有单词没有序号\n",
    "# vocab2id: 249536 (单词, 序号)\n",
    "id2vocab = TEXT.vocab.itos\n",
    "vocab2id = TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28156ec1-4332-4bb5-9419-bb104d20671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = vocab2id[TEXT.unk_token]   # 0\n",
    "PAD_IDX = vocab2id[TEXT.pad_token]   # 1\n",
    "SOS_IDX = vocab2id[TEXT.init_token]  # 2\n",
    "EOS_IDX = vocab2id[TEXT.eos_token]   # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3feae429-3d8d-4a36-b42b-a40d47b4cd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2192d019-16b5-49a5-9424-26073e27f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch0 torch.Size([64, 37])\n",
      "train_batch1 torch.Size([64, 40])\n",
      "train_batch2 torch.Size([64, 41])\n",
      "train_batch3 torch.Size([64, 38])\n",
      "train_batch4 torch.Size([64, 49])\n",
      "train_batch5 torch.Size([64, 38])\n",
      "train_batch6 torch.Size([64, 37])\n",
      "train_batch7 torch.Size([64, 37])\n",
      "train_batch8 torch.Size([64, 37])\n",
      "train_batch9 torch.Size([64, 38])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_batch = next(iter(train_iter))\n",
    "    print('train_batch' + str(i), train_batch.trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0b2609-58bf-45a7-a79f-092855e5ee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in range(len(train_iter)):\n",
    "    j += 1\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "312522ce-880f-47f7-a626-4019fba6e22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,   278,    21,  ...,  2088,     6,     3],\n",
       "        [    2,  8863,    88,  ...,     1,     1,     1],\n",
       "        [    2,  2714,   101,  ...,    95,   589,    59],\n",
       "        ...,\n",
       "        [    2, 22988, 12035,  ...,     1,     1,     1],\n",
       "        [    2,  3011,    14,  ...,   472,  1435,   742],\n",
       "        [    2,  1483,    88,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch = next(iter(train_iter))\n",
    "train_batch.trg[:,:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4030e256-a8ea-4658-8c9f-e1cc6b9c502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,   135,   904,  ...,     1,     1,     1],\n",
       "        [    2,  1747,  7424,  ...,     1,     1,     1],\n",
       "        [    2,   142,    18,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    2,   751,   162,  ...,     1,     1,     1],\n",
       "        [    2, 60161,   303,  ...,  2214, 22848,     3],\n",
       "        [    2,   385, 19670,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c1494-92ec-4166-b344-9f0777958aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424c002-19f7-44d9-aa9e-6b503929b757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b1ff2e4-6882-485a-90ac-d84b11f8b51c",
   "metadata": {},
   "source": [
    "## **定义模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3335a4c7-ec26-45a8-9362-2f258513e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51962021-0401-4341-a2cf-5c9a934a141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        # 批量放在前面\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # 用词向量更新隐藏状态\n",
    "    # 只返回\n",
    "    def forward(self, src):\n",
    "        # src = [batch size, src len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch size, src len, emb dim]\n",
    "        outputs, state = self.rnn(embedded)\n",
    "        # outputs = [batch size, src len, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84ea7f7-17be-4c43-93bc-94b5b399f4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=10, emb_dim=8, hid_dim=16,\n",
    "                         n_layers=2, dropout=0.5)\n",
    "encoder.eval()\n",
    "# batch_size = 4, seq_len = 7\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b4690e-7eb0-4f71-83d6-27c8eb9ba4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b130be-393e-4a73-aaf1-5530245e5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器。\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout, batch_first=True)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def init_state(self, enc_outputs):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(`batch_size`, `num_steps`, `embed_size`)\n",
    "        X = self.embedding(X)\n",
    "        # 广播`context`，使其具有与`X`相同的`num_steps`\n",
    "        # context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        # X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X, state)\n",
    "        output = self.dense(output)\n",
    "        # `output`的形状: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "        # `state[0]`的形状: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7f14d1-6270-4512-9ec7-3719f2e0241f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1457a5a-012e-477e-b966-96dea5d55f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        state = self.decoder.init_state(encoder(src))\n",
    "        output, state = self.decoder(trg, state)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a050cf-3065-4516-b43d-bd80a3daa17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4919a3e-e5cc-46c1-81f2-1ba50c1b2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "loss_vals = []\n",
    "loss_vals_eval = []\n",
    "\n",
    "\n",
    "def train_seq2seq(net, train_iter, lr, num_epochs):\n",
    "    \"\"\"训练序列到序列模型。\"\"\"\n",
    "    net.apply(xavier_init_weights)\n",
    "    # net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # loss = MaskedSoftmaxCELoss()\n",
    "    \n",
    "    net.train()\n",
    "    m = len(train_iter)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        net.zero_grad()\n",
    "        epoch_loss = []\n",
    "        for i in range(m):\n",
    "            batch = next(iter(train_iter))\n",
    "            output = net(batch.src, batch.trg)   \n",
    "            # if i < 10 and epoch==0:\n",
    "            #     print('output.shape=======>', output.shape)    # output的形状：(batch_size, num_steps, vocab_size)\n",
    "            #     print('batch.trg==========>', batch.trg.shape) # batch.trg的形状: (batch_size, num_steps)\n",
    "            loss = criterion(output.permute(0, 2, 1), batch.trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "            epoch_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "            print('epoch' + str(epoch) + '样本' + str(i) + '==========>', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb234bf-52c0-4369-ba68-4162ead54aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(id2vocab)\n",
    "output_dim = len(id2vocab)\n",
    "emd_size = 256\n",
    "num_hids = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "num_epoches = 10\n",
    "clip = 1\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce392e4-56ba-4016-9eb0-79e22ed51220",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(id2vocab), emd_size, num_hids, n_layers,\n",
    "                        dropout)\n",
    "decoder = Decoder(len(id2vocab), emd_size, num_hids, n_layers,\n",
    "                        dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3452a2-80b0-4fda-9d67-4e30f301a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "net = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "781f3001-da60-4342-bddd-37d934acc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0样本0==========> tensor(12.4274, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本1==========> tensor(12.2757, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本2==========> tensor(10.8230, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本3==========> tensor(9.4668, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本4==========> tensor(9.8795, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本5==========> tensor(10.0311, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本6==========> tensor(10.0333, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本7==========> tensor(10.0766, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本8==========> tensor(10.0511, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本9==========> tensor(9.7745, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本10==========> tensor(9.6329, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本11==========> tensor(8.9345, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本12==========> tensor(8.9986, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本13==========> tensor(8.8704, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本14==========> tensor(9.3262, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本15==========> tensor(9.3859, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本16==========> tensor(9.6258, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本17==========> tensor(9.5413, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本18==========> tensor(9.2904, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本19==========> tensor(9.6412, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本20==========> tensor(9.4642, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本21==========> tensor(9.3376, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本22==========> tensor(9.8136, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本23==========> tensor(9.4595, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本24==========> tensor(9.1453, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本25==========> tensor(9.0736, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本26==========> tensor(8.9483, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本27==========> tensor(8.9460, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本28==========> tensor(8.9062, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本29==========> tensor(8.5114, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本30==========> tensor(8.5224, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本31==========> tensor(8.2202, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本32==========> tensor(8.4209, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本33==========> tensor(8.7707, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本34==========> tensor(8.4858, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本35==========> tensor(8.1855, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本36==========> tensor(8.1873, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本37==========> tensor(7.9320, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本38==========> tensor(8.0236, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本39==========> tensor(7.7018, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本40==========> tensor(7.7990, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本41==========> tensor(8.0501, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本42==========> tensor(7.7351, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本43==========> tensor(7.6005, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本44==========> tensor(7.9056, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本45==========> tensor(7.6683, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本46==========> tensor(7.7789, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本47==========> tensor(7.5922, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本48==========> tensor(7.7220, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本49==========> tensor(7.5755, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本50==========> tensor(7.5538, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本51==========> tensor(7.3508, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本52==========> tensor(7.4997, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本53==========> tensor(7.3127, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本54==========> tensor(7.4346, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本55==========> tensor(7.1444, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本56==========> tensor(7.2155, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本57==========> tensor(7.2213, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本58==========> tensor(7.0084, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本59==========> tensor(6.9901, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本60==========> tensor(6.8007, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本61==========> tensor(6.7064, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本62==========> tensor(6.8826, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本63==========> tensor(6.7219, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本64==========> tensor(6.6363, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本65==========> tensor(6.7066, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本66==========> tensor(6.4933, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本67==========> tensor(6.2462, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本68==========> tensor(6.3662, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本69==========> tensor(6.2274, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本70==========> tensor(6.4501, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本71==========> tensor(6.4807, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本72==========> tensor(6.2268, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本73==========> tensor(6.1236, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本74==========> tensor(6.0166, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本75==========> tensor(6.0967, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本76==========> tensor(5.7271, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本77==========> tensor(6.0280, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本78==========> tensor(5.8173, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本79==========> tensor(5.8119, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本80==========> tensor(5.5800, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本81==========> tensor(5.8637, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本82==========> tensor(5.6551, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本83==========> tensor(5.7387, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本84==========> tensor(5.6827, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本85==========> tensor(5.7099, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本86==========> tensor(5.6081, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本87==========> tensor(5.8737, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本88==========> tensor(5.2605, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本89==========> tensor(5.5524, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本90==========> tensor(5.2439, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本91==========> tensor(5.4217, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本92==========> tensor(5.3451, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本93==========> tensor(5.3252, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本94==========> tensor(5.3163, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本95==========> tensor(4.8464, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本96==========> tensor(4.9392, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本97==========> tensor(5.1576, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本98==========> tensor(5.2435, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本99==========> tensor(4.6495, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本100==========> tensor(5.0794, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本101==========> tensor(4.6476, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本102==========> tensor(4.8353, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本103==========> tensor(4.5970, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本104==========> tensor(4.9465, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本105==========> tensor(4.5799, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本106==========> tensor(4.3038, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本107==========> tensor(4.3480, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本108==========> tensor(4.9485, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本109==========> tensor(4.7174, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本110==========> tensor(4.4542, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本111==========> tensor(4.2571, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本112==========> tensor(4.3360, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本113==========> tensor(4.4204, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本114==========> tensor(4.4631, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本115==========> tensor(4.6509, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本116==========> tensor(4.5064, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本117==========> tensor(4.1538, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本118==========> tensor(4.4262, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本119==========> tensor(4.2484, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本120==========> tensor(4.3397, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本121==========> tensor(4.1389, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本122==========> tensor(4.2456, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本123==========> tensor(4.2051, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本124==========> tensor(4.3939, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本125==========> tensor(4.3476, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本126==========> tensor(3.5866, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本127==========> tensor(3.7324, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本128==========> tensor(3.9311, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本129==========> tensor(3.8921, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本130==========> tensor(3.7936, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本131==========> tensor(3.8728, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本132==========> tensor(3.9604, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本133==========> tensor(4.2109, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本134==========> tensor(3.9727, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本135==========> tensor(3.8906, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本136==========> tensor(3.8160, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本137==========> tensor(3.9987, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本138==========> tensor(3.7350, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本139==========> tensor(3.5499, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本140==========> tensor(3.5693, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本141==========> tensor(3.7645, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本142==========> tensor(3.7736, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本143==========> tensor(3.4828, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本144==========> tensor(3.6863, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本145==========> tensor(3.5205, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本146==========> tensor(3.3147, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本147==========> tensor(3.0115, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本148==========> tensor(3.9202, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本149==========> tensor(3.3065, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本150==========> tensor(3.5152, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本151==========> tensor(3.2982, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本152==========> tensor(3.8962, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本153==========> tensor(3.4887, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本154==========> tensor(3.3747, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本155==========> tensor(3.5254, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本156==========> tensor(3.3816, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本157==========> tensor(3.3619, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本158==========> tensor(3.3191, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本159==========> tensor(3.1272, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本160==========> tensor(3.2922, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本161==========> tensor(3.4771, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本162==========> tensor(3.2718, grad_fn=<NllLoss2DBackward0>)\n",
      "epoch0样本163==========> tensor(3.5073, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 131072 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_540/3163542691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_seq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_540/1342870609.py\u001b[0m in \u001b[0;36mtrain_seq2seq\u001b[1;34m(net, train_iter, lr, num_epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;31m# if i < 10 and epoch==0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m#     print('output.shape=======>', output.shape)    # output的形状：(batch_size, num_steps, vocab_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SOFT_WARE\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_540/326248595.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SOFT_WARE\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_540/3281801956.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# embedded = [batch size, src len, emb dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# outputs = [batch size, src len, hid dim * n directions]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# hidden = [n layers * n directions, batch size, hid dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SOFT_WARE\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SOFT_WARE\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    850\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    851\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 131072 bytes."
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "train_seq2seq(net, train_iter, lr, num_epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69847388-6e51-461d-9b20-b7cfa9d1156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'Seq2Seq.pt')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740b67b-8747-435d-87ea-067229ca06e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
